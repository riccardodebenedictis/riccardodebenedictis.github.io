---
layout: post
title: Dialogue is all you need!
---

[Transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) are a powerful architecture, which can be used for a wide range of tasks, including machine translation, text summarization, and dialogue generation. The Transformer architecture is a stack of self-attention layers, which are followed by feed-forward layers. The self-attention layers are based on the attention mechanism, which is a mechanism that allows a model to focus on a specific part of the input sequence, while generating the output sequence. They have been introduced in a famous paper entitled "Attention is all you need" \[[1](#r1)\].

These architectures have become particularly famous, in recent times, thanks to their use within [ChatGPT](https://chat.openai.com/), a chatbot that, trained on a large corpus of text, is able to predict the next word in a sequence, given the previous words in the sequence, resulting in the generation of coherent and entertaining conversations.

The criticisms of these systems, however, are not few. Due to its structure, in fact, by simply choosing sequences of words, one runs the risk that the system [invents its own facts](https://twitter.com/itstimconnors/status/1599544717943123969?s=20&t=cAB9lIKbw8q5DQ2svACxgQ) and, therefore, is increasingly being [excluded](https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned) from critical situations such as responding convincingly, but often incorrectly, to questions. Not to mention the fact that it risks [amplifying some biases](https://www.bloomberg.com/news/newsletters/2022-12-08/chatgpt-open-ai-s-chatbot-is-spitting-out-biased-sexist-results) already present in the users of the system. Furthermore, relying almost exclusively on natural language, the logical and mathematical abilities are [extremely limited](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/).

Yet, the fact that ChatGPT has become so popular, despite the above limitations, is a clear indication that the use of natural language is still the most effective way to interact with machines. Based on this assumption, we therefore proposed, in a recent paper \[[2](#r2)\], an adaptation of the [three-tier architecture](/research/) in which the policy of the sequencing tier is based on a Transformer architecture. The novelty of this approach is that the Transformer architecture is used to generate the sequence of actions to be executed by the robot, rather than the sequence of words to be spoken, which are more coherent and more consistent with the context.

The architecture has been adapted to enhance the capabilities of mobile telepresence robots (see [Enhanced Telepresence](/enhanced_telepresence/)), which are a class of robots that can be remotely controlled by a human operator. Such robots are typically equipped with a camera, which allows them to perceive the environment, and a microphone, which allows them to perceive the speech of the human operator. The robots are also equipped with a speaker, which allows them to communicate with the human operator, and a set of actuators, which allow them to move in the environment. Given their semplicity, these robots are very cost-effective and can be used in a wide range of applications, including the monitoring of older adults living alone. Their standards capabilities, however, are very limited. Through the proposed architecture, hence, we aim to enhance the capabilities of these robots by allowing them to autonomously perform a wider set of actions, without the need for human intervention.

Executable actions include moving to a specific location, changing the facial expression of the robot, uttering a specific sentence and, perhaps the most interesting one, asking to the deliberative tier to update the currently executing plan.

### References

[<a name="r1"></a>1] Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A. N., Kaiser Ł., and Polosukhin I. (2017), *Attention is all you need*. Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 6000–6010. [Online]. Available: http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.

[<a name="r2"></a>2] De Benedictis, R., Beraldo, G., Cortellessa, G., Fracasso, F., Cesta, A. (2022). A Transformer-Based Approach for Choosing Actions in Social Robotics. In: Cavallo, et al. Social Robotics. ICSR 2022. Lecture Notes in Computer Science(), vol 13817. Springer, Cham. https://doi.org/10.1007/978-3-031-24667-8_18.